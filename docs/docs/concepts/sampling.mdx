---
title: "Sampling" | "采样"
description: "Let your servers request completions from LLMs" | "让您的服务器通过客户端请求LLM补全"
---

Sampling is a powerful MCP feature that allows servers to request LLM completions through the client, enabling sophisticated agentic behaviors while maintaining security and privacy. | 采样是MCP的一个强大功能，它允许服务器通过客户端请求LLM补全，在保持安全性和隐私性的同时实现复杂的代理行为。

<Info>
  This feature of MCP is not yet supported in the Claude Desktop client. | MCP的这一功能目前在Claude Desktop客户端中尚未支持。
</Info>

## How sampling works | 采样工作原理

The sampling flow follows these steps: | 采样流程遵循以下步骤：

1. Server sends a `sampling/createMessage` request to the client | 服务器向客户端发送`sampling/createMessage`请求
2. Client reviews the request and can modify it | 客户端审查请求并可以修改它
3. Client samples from an LLM | 客户端从LLM进行采样
4. Client reviews the completion | 客户端审查补全结果
5. Client returns the result to the server | 客户端将结果返回给服务器

This human-in-the-loop design ensures users maintain control over what the LLM sees and generates. | 这种人机协作的设计确保用户能够控制LLM看到和生成的内容。

## Message format | 消息格式

Sampling requests use a standardized message format: | 采样请求使用标准化的消息格式：

```typescript
{
  messages: [
    {
      role: "user" | "assistant",
      content: {
        type: "text" | "image",

        // For text: | 对于文本：
        text?: string,

        // For images: | 对于图像：
        data?: string,             // base64 encoded | base64编码
        mimeType?: string
      }
    }
  ],
  modelPreferences?: {
    hints?: [{
      name?: string                // Suggested model name/family | 建议的模型名称/系列
    }],
    costPriority?: number,         // 0-1, importance of minimizing cost | 0-1，最小化成本的重要性
    speedPriority?: number,        // 0-1, importance of low latency | 0-1，低延迟的重要性
    intelligencePriority?: number  // 0-1, importance of capabilities | 0-1，能力的重要性
  },
  systemPrompt?: string,
  includeContext?: "none" | "thisServer" | "allServers",
  temperature?: number,
  maxTokens: number,
  stopSequences?: string[],
  metadata?: Record<string, unknown>
}
```

## Request parameters | 请求参数

### Messages | 消息

The `messages` array contains the conversation history to send to the LLM. Each message has: | `messages`数组包含要发送给LLM的对话历史。每条消息包含：

- `role`: Either "user" or "assistant" | `role`：可以是"user"或"assistant"
- `content`: The message content, which can be: | `content`：消息内容，可以是：
  - Text content with a `text` field | 带有`text`字段的文本内容
  - Image content with `data` (base64) and `mimeType` fields | 带有`data`（base64）和`mimeType`字段的图像内容

### Model preferences | 模型偏好

The `modelPreferences` object allows servers to specify their model selection preferences: | `modelPreferences`对象允许服务器指定其模型选择偏好：

- `hints`: Array of model name suggestions that clients can use to select an appropriate model: | `hints`：客户端可以用来选择合适模型的模型名称建议数组：
  - `name`: String that can match full or partial model names (e.g. "claude-3", "sonnet") | `name`：可以匹配完整或部分模型名称的字符串（例如"claude-3"、"sonnet"）
  - Clients may map hints to equivalent models from different providers | 客户端可以将提示映射到不同提供商的等效模型
  - Multiple hints are evaluated in preference order | 多个提示按优先级顺序评估

- Priority values (0-1 normalized): | 优先级值（0-1标准化）：
  - `costPriority`: Importance of minimizing costs | `costPriority`：最小化成本的重要性
  - `speedPriority`: Importance of low latency response | `speedPriority`：低延迟响应的重要性
  - `intelligencePriority`: Importance of advanced model capabilities | `intelligencePriority`：高级模型能力的重要性

Clients make the final model selection based on these preferences and their available models. | 客户端根据这些偏好和可用模型做出最终的模型选择。

### System prompt | 系统提示

An optional `systemPrompt` field allows servers to request a specific system prompt. The client may modify or ignore this. | 可选的`systemPrompt`字段允许服务器请求特定的系统提示。客户端可以修改或忽略此提示。

### Context inclusion | 上下文包含

The `includeContext` parameter specifies what MCP context to include: | `includeContext`参数指定要包含的MCP上下文：

- `"none"`: No additional context | `"none"`：不包含额外上下文
- `"thisServer"`: Include context from the requesting server | `"thisServer"`：包含来自请求服务器的上下文
- `"allServers"`: Include context from all connected MCP servers | `"allServers"`：包含所有连接的MCP服务器的上下文

The client controls what context is actually included. | 客户端控制实际包含的上下文。

### Sampling parameters | 采样参数

Fine-tune the LLM sampling with: | 使用以下参数微调LLM采样：

- `temperature`: Controls randomness (0.0 to 1.0) | `temperature`：控制随机性（0.0到1.0）
- `maxTokens`: Maximum tokens to generate | `maxTokens`：生成的最大token数
- `stopSequences`: Array of sequences that stop generation | `stopSequences`：停止生成的序列数组
- `metadata`: Additional provider-specific parameters | `metadata`：额外的提供商特定参数

## Response format | 响应格式

The client returns a completion result: | 客户端返回补全结果：

```typescript
{
  model: string,  // Name of the model used | 使用的模型名称
  stopReason?: "endTurn" | "stopSequence" | "maxTokens" | string,
  role: "user" | "assistant",
  content: {
    type: "text" | "image",
    text?: string,
    data?: string,
    mimeType?: string
  }
}
```

## Example request | 示例请求

Here's an example of requesting sampling from a client: | 这是一个从客户端请求采样的示例：
```json
{
  "method": "sampling/createMessage",
  "params": {
    "messages": [
      {
        "role": "user",
        "content": {
          "type": "text",
          "text": "What files are in the current directory?"
        }
      }
    ],
    "systemPrompt": "You are a helpful file system assistant.",
    "includeContext": "thisServer",
    "maxTokens": 100
  }
}
```

## Best practices | 最佳实践

When implementing sampling: | 在实现采样时：

1. Always provide clear, well-structured prompts | 始终提供清晰、结构良好的提示
2. Handle both text and image content appropriately | 适当处理文本和图像内容
3. Set reasonable token limits | 设置合理的token限制
4. Include relevant context through `includeContext` | 通过`includeContext`包含相关上下文
5. Validate responses before using them | 在使用响应之前验证它们
6. Handle errors gracefully | 优雅地处理错误
7. Consider rate limiting sampling requests | 考虑对采样请求进行速率限制
8. Document expected sampling behavior | 记录预期的采样行为
9. Test with various model parameters | 使用各种模型参数进行测试
10. Monitor sampling costs | 监控采样成本

## Human in the loop controls | 人机协作控制

Sampling is designed with human oversight in mind: | 采样设计考虑了人工监督：

### For prompts | 对于提示

- Clients should show users the proposed prompt | 客户端应向用户显示建议的提示
- Users should be able to modify or reject prompts | 用户应该能够修改或拒绝提示
- System prompts can be filtered or modified | 系统提示可以被过滤或修改
- Context inclusion is controlled by the client | 上下文包含由客户端控制

### For completions | 对于补全

- Clients should show users the completion | 客户端应向用户显示补全结果
- Users should be able to modify or reject completions | 用户应该能够修改或拒绝补全结果
- Clients can filter or modify completions | 客户端可以过滤或修改补全结果
- Users control which model is used | 用户控制使用哪个模型

## Security considerations | 安全考虑

When implementing sampling: | 在实现采样时：

- Validate all message content | 验证所有消息内容
- Sanitize sensitive information | 清理敏感信息
- Implement appropriate rate limits | 实施适当的速率限制
- Monitor sampling usage | 监控采样使用情况
- Encrypt data in transit | 加密传输中的数据
- Handle user data privacy | 处理用户数据隐私
- Audit sampling requests | 审计采样请求
- Control cost exposure | 控制成本暴露
- Implement timeouts | 实施超时
- Handle model errors gracefully | 优雅地处理模型错误

## Common patterns | 常见模式

### Agentic workflows | 代理工作流

Sampling enables agentic patterns like: | 采样支持以下代理模式：

- Reading and analyzing resources | 读取和分析资源
- Making decisions based on context | 基于上下文做出决策
- Generating structured data | 生成结构化数据
- Handling multi-step tasks | 处理多步骤任务
- Providing interactive assistance | 提供交互式帮助

### Context management | 上下文管理

Best practices for context: | 上下文的最佳实践：

- Request minimal necessary context | 请求最小必要的上下文
- Structure context clearly | 清晰地构建上下文
- Handle context size limits | 处理上下文大小限制
- Update context as needed | 根据需要更新上下文
- Clean up stale context | 清理过时的上下文

### Error handling | 错误处理

Robust error handling should: | 健壮的错误处理应该：

- Catch sampling failures | 捕获采样失败
- Handle timeout errors | 处理超时错误
- Manage rate limits | 管理速率限制
- Validate responses | 验证响应
- Provide fallback behaviors | 提供回退行为
- Log errors appropriately | 适当记录错误

## Limitations | 限制

Be aware of these limitations: | 请注意以下限制：

- Sampling depends on client capabilities | 采样依赖于客户端能力
- Users control sampling behavior | 用户控制采样行为
- Context size has limits | 上下文大小有限制
- Rate limits may apply | 可能适用速率限制
- Costs should be considered | 应考虑成本
- Model availability varies | 模型可用性各不相同
- Response times vary | 响应时间各不相同
- Not all content types supported | 并非所有内容类型都受支持
